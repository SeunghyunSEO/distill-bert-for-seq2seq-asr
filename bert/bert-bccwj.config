[model]
hidden_size = 512
num_hidden_layers = 6
num_attention_heads = 8
intermediate_size = 3072
max_position_embeddings = 256

[data]
train_dir = ../data/bccwj.train.s256
train_size = 1198917

[vocab]
unk_id = 1
mask_id = 4
vocab_size = 7520

[log]
log_path = ./log/pretrain.bert.bccwj.log
log_step = 100

[save]
save_prefix = ./checkpoints/bert.bccwj
save_epoch = 1

[train]
batch_size = 150
num_epochs = 50
learning_rate = 1e-4
warmup_proportion = 0.1
weight_decay = 0.01

[mask]
num_to_mask = 20
max_seq_len = 256
